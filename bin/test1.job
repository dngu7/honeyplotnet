#!/bin/bash
#rj name=test_continuous queue=csiro_od210966 priority=100 runtime=1 nodes=1 mem=64g taskspernode=4 features=v100x4

set -euo pipefail

module add openmpi/4.1.3-mlnx-icc

export DATA_HOME=/data/csiro_od210966
export CODE_DIR=$DATA_HOME/code/dvq_charts
export CFLAGS="-I$DATA_HOME/envs/libraries/libaio-0.3.112/usr/include"
export LDFLAGS="-L$DATA_HOME/envs/libraries/libaio-0.3.112/usr/lib"
export CC=/d/sw/gcc/9.2.0/bin/gcc
export CXX=/d/sw/gcc/9.2.0/bin/g++
export TMPDIR="$TMPDIR_SHM"

export SLURM_CPUS_PER_TASK=8
export GPUS=4

export NCCL_IB_DISABLE=1
export NCCL_SOCKET_IFNAME=eth0

CONFIG=continuous-mvq.yaml
STAGE=continuous
WORK_ENV=dug
MODE=train

#Load environments
source $PWD/env.sh

NODE_LIST_LONG=$(scontrol show hostnames "$SLURM_JOB_NODELIST")
echo "NODE_LIST_LONG=$NODE_LIST_LONG"

# The first hostname is the master address
master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
#master_addr=$("$NODE_LIST_LONG" | head -n 1)
export MASTER_ADDR=$master_addr.dug.com
echo HOST=$SLURM_SUBMIT_HOST JOBID=$SLURM_JOB_ID \| MASTER_ADDR=$MASTER_ADDR \| NODELIST=$SLURM_NODELIST \| NTASKS_PER_NODE=$SLURM_NTASKS_PER_NODE \| SLURM_CPUS_ON_NODE=$SLURM_CPUS_ON_NODE

cd $CODE_DIR

export TMPDIR="$TMPDIR_SHM"

MASTER_ADDR="localhost:0"
NNODES=1
GPUS=4

torchrun --standalone --nnodes=$NNODES \
    --nproc_per_node=$GPUS \
    --max_restarts=1 \
    --rdzv_id=$SLURM_JOB_ID \
    --rdzv_backend=c10d \
    --rdzv_endpoint=$MASTER_ADDR \
    main.py -w $WORK_ENV -c $CONFIG -m $MODE -s $STAGE

exit 0