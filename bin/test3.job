#!/bin/bash
#rj name=test_charttext queue=csiro_od210966 priority=100 runtime=48 nodes=1 mem=64g taskspernode=4 features=a100x4

set -euo pipefail

module add openmpi/4.1.3-mlnx-icc

export DATA_HOME=/data/csiro_od210966
export CODE_DIR=$DATA_HOME/code/dvq_charts
export CFLAGS="-I$DATA_HOME/envs/libraries/libaio-0.3.112/usr/include"
export LDFLAGS="-L$DATA_HOME/envs/libraries/libaio-0.3.112/usr/lib"
export CC=/d/sw/gcc/9.2.0/bin/gcc
export CXX=/d/sw/gcc/9.2.0/bin/g++
export TMPDIR="$TMPDIR_SHM"

export SLURM_CPUS_PER_TASK=8
export GPUS=4
export NCCL_IB_DISABLE=1
export NCCL_SOCKET_IFNAME=eth0

export TRANSFORMERS_CACHE=/data/csiro_od210966/data/cache

CONFIG=caption-bird.yaml 
STAGE=chart_text 
WORK=dug
MODE=train

DS_CONFIG_DIR=$CODE_DIR/config/ds
DS_CONFIG="$DS_CONFIG_DIR/default.json"
HOSTFILE="$CODE_DIR/bin/dug/hf/hf-$SLURM_JOB_ID" 

source $PWD/env.sh

echo $HOSTFILE
rm -f $HOSTFILE

#DEBUGGING
#export CUDA_LAUNCH_BLOCKING=1
#export NCCL_DEBUG=INFO
#export NCCL_DEBUG_SUBSYS=ALL

#Load environments

NODE_LIST_LONG=$(scontrol show hostnames "$SLURM_JOB_NODELIST")
echo "NODE_LIST_LONG=$NODE_LIST_LONG"

#IFS='\n' read -ra node_list <<< "$NODE_LIST_LONG"
#for worker in "${node_list[@]}"
for node in $NODE_LIST_LONG
do
  echo "$node.dug.com slots=4"
  if [ ! -e $HOSTFILE ]; then
    echo "$node slots=4" > $HOSTFILE
  else
    echo "$node slots=4" >> $HOSTFILE
  fi
done

# python -c 'import deepspeed; deepspeed.ops.adam.cpu_adam.CPUAdamBuilder().load()'

# The first hostname is the master address
master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
#master_addr=$("$NODE_LIST_LONG" | head -n 1)
export MASTER_ADDR=$master_addr.dug.com
echo HOST=$SLURM_SUBMIT_HOST JOBID=$SLURM_JOB_ID \| MASTER_ADDR=$MASTER_ADDR \| NODELIST=$SLURM_NODELIST \| NTASKS_PER_NODE=$SLURM_NTASKS_PER_NODE \| SLURM_CPUS_ON_NODE=$SLURM_CPUS_ON_NODE

cd $CODE_DIR

set TMPDIR="$TMPDIR_SHM"

deepspeed main.py -c $CONFIG -w $WORK -s $STAGE  -m $MODE 
